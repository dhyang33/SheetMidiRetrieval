{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter, ImageChops\n",
    "import cv2\n",
    "from skimage import filters, measure\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import label2rgb\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.spatial import KDTree\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import librosa as lb\n",
    "import time\n",
    "import cProfile\n",
    "import os\n",
    "import os.path\n",
    "import pyximport; pyximport.install()\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefile = 'data/queries/p95_q8.jpg'\n",
    "midi_db_dir = 'experiments/baseline/db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### system parameters ###\n",
    "\n",
    "# Pre-processing\n",
    "#resizeW = 1000 # initial resize\n",
    "#resizeH = 1000\n",
    "thumbnailW = 100  # bkgd lighting\n",
    "thumbnailH = 100\n",
    "thumbnailFilterSize = 5\n",
    "estLineSep_NumCols = 3\n",
    "estLineSep_LowerRange = 25\n",
    "estLineSep_UpperRange = 45\n",
    "estLineSep_Delta = 1\n",
    "targetLineSep = 10.0\n",
    "\n",
    "# Staff Line Features\n",
    "morphFilterHorizLineSize = 51\n",
    "notebarFiltLen = 3\n",
    "notebarRemoval = 0.9\n",
    "calcStaveFeatureMap_NumCols = 10\n",
    "calcStaveFeatureMap_LowerRange = 8.5\n",
    "calcStaveFeatureMap_UpperRange = 11.75\n",
    "calcStaveFeatureMap_Delta = 0.25\n",
    "\n",
    "# Notehead Detection\n",
    "morphFilterCircleSizeReduce = 5\n",
    "morphFilterCircleSizeExpand = 5\n",
    "#morphFilterCircleSize = 5\n",
    "notedetect_minarea = 50\n",
    "notedetect_maxarea = 200 \n",
    "noteTemplateSize = 21\n",
    "notedetect_tol_ratio = .4\n",
    "\n",
    "# temporary\n",
    "chordBlock_minH = 1.8\n",
    "chordBlock_maxH = 4.25\n",
    "chordBlock_minW = .8\n",
    "chordBlock_maxW = 1.25\n",
    "chordBlock_minArea = 1.8\n",
    "chordBlock_maxArea = 4.5\n",
    "chordBlock_minNotes = 2\n",
    "chordBlock_maxNotes = 4\n",
    "\n",
    "# Staffline Detection\n",
    "maxDeltaRowInitial = 50\n",
    "minNumStaves = 2\n",
    "maxNumStaves = 12\n",
    "minStaveSeparation = 6 * targetLineSep\n",
    "maxDeltaRowRefined = 15\n",
    "\n",
    "# Group Staves\n",
    "morphFilterVertLineLength = 101\n",
    "morphFilterVertLineWidth = 7\n",
    "#maxBarlineLenFactor = .25\n",
    "#maxBarlineWidth = 10\n",
    "\n",
    "# Generate Bootleg Score\n",
    "bootlegRepeatNotes = 2 \n",
    "bootlegFiller = 1\n",
    "\n",
    "# Alignment\n",
    "dtw_steps = [1,1,1,2,2,1] # dtw\n",
    "dtw_weights = [1,1,2]\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing consists of two steps:\n",
    "- background subtraction to reduce effect of lighting conditions\n",
    "- interline normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim1 = Image.open(imagefile).convert('L') # pim indicates PIL image object, im indicates raw pixel values\n",
    "#pim1.thumbnail([resizeW, resizeH]) # modifies in place\n",
    "#pim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBkgdLighting(pimg, filtsz=5, thumbnailW = 100, thumbnailH = 100):\n",
    "    tinyimg = pimg.copy()\n",
    "    tinyimg.thumbnail([thumbnailW, thumbnailH]) # resize to speed up\n",
    "    #shadows = tinyimg.filter(ImageFilter.MaxFilter(filtsz)).resize(pimg.size)\n",
    "    shadows = tinyimg.filter(ImageFilter.GaussianBlur(filtsz)).resize(pimg.size)\n",
    "    result = ImageChops.invert(ImageChops.subtract(shadows, pimg))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim2 = removeBkgdLighting(pim1, thumbnailFilterSize, thumbnailW, thumbnailH)\n",
    "pim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPenalizedCombFilter(linesep):\n",
    "    filt = np.zeros(int(np.round(linesep * 5)))\n",
    "    \n",
    "    # positive spikes\n",
    "    for i in range(5):\n",
    "        offset = int(np.round(.5*linesep + i*linesep))\n",
    "        filt[offset-1:offset+2] = 1.0\n",
    "    \n",
    "    # negative spikes\n",
    "    for i in range(6):\n",
    "        center = int(np.round(i*linesep))\n",
    "        startIdx = max(center - 1, 0)\n",
    "        endIdx = min(center + 2, len(filt))\n",
    "        filt[startIdx:endIdx] = -1.0\n",
    "        \n",
    "    return filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateLineSep(pim, ncols, lrange, urange, delta):\n",
    "    \n",
    "    # break image into columns, calculate row medians for inner columns (exclude outermost columns)\n",
    "    img = 255 - np.array(pim)\n",
    "    imgHeight, imgWidth = img.shape\n",
    "    rowMedians = np.zeros((imgHeight, ncols))\n",
    "    colWidth = imgWidth // (ncols + 2)\n",
    "    for i in range(ncols):\n",
    "        rowMedians[:,i] = np.median(img[:,(i+1)*colWidth:(i+2)*colWidth], axis=1)\n",
    "    \n",
    "    # apply comb filters\n",
    "    lineseps = np.arange(lrange, urange, delta)\n",
    "    responses = np.zeros((len(lineseps), imgHeight, ncols))\n",
    "    for i, linesep in enumerate(lineseps):\n",
    "        filt = getPenalizedCombFilter(linesep).reshape((-1,1))\n",
    "        responses[i,:,:] = convolve2d(rowMedians, filt, mode = 'same')\n",
    "    \n",
    "    # find comb filter with strongest response\n",
    "    scores = np.sum(np.max(responses, axis=1), axis=1)\n",
    "    bestIdx = np.argmax(scores)\n",
    "    estLineSep = lineseps[bestIdx]\n",
    "    \n",
    "    return estLineSep, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pim = pim2\n",
    "# ncols = 3\n",
    "# lrange = 25\n",
    "# urange = 45\n",
    "# delta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = 255 - np.array(pim)\n",
    "# imgHeight, imgWidth = img.shape\n",
    "# rowMedians = np.zeros((imgHeight, ncols))\n",
    "# colWidth = imgWidth // (ncols + 2)\n",
    "# for i in range(ncols):\n",
    "#     rowMedians[:,i] = np.median(img[:,(i+1)*colWidth:(i+2)*colWidth], axis=1)\n",
    "\n",
    "# # apply comb filters\n",
    "# lineseps = np.arange(lrange, urange, delta)\n",
    "# responses = np.zeros((len(lineseps), imgHeight, ncols))\n",
    "# for i, linesep in enumerate(lineseps):\n",
    "#     filt = getPenalizedCombFilter(linesep).reshape((-1,1))\n",
    "#     responses[i,:,:] = convolve2d(rowMedians, filt, mode = 'same')\n",
    "\n",
    "# # find comb filter with strongest response\n",
    "# scores = np.sum(np.max(responses, axis=1), axis=1)\n",
    "# bestIdx = np.argmax(scores)\n",
    "# estLineSep = lineseps[bestIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesep, scores = estimateLineSep(pim2, estLineSep_NumCols, estLineSep_LowerRange, estLineSep_UpperRange, estLineSep_Delta)\n",
    "linesep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResizedDimensions(pim, estimatedLineSep, desiredLineSep):\n",
    "    curH, curW = pim.height, pim.width\n",
    "    scale_factor = 1.0 * desiredLineSep / estimatedLineSep\n",
    "    targetH = int(curH * scale_factor)\n",
    "    targetW = int(curW * scale_factor)    \n",
    "    return targetH, targetW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetH, targetW = calcResizedDimensions(pim2, linesep, targetLineSep)\n",
    "pim2 = pim2.resize((targetW, targetH))\n",
    "targetH, targetW, pim1.height, pim1.width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staff Line Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormImage(img):\n",
    "    X = 1 - np.array(img) / 255.0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGrayscaleImage(X, sz = (10,10), maxval = 1, inverted = True):\n",
    "    # by default assumes X is a normalized image between 0 (white) and 1 (black)\n",
    "    plt.figure(figsize = sz)\n",
    "    if inverted:\n",
    "        plt.imshow(maxval-X, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(X, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = getNormImage(pim2)\n",
    "showGrayscaleImage(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def morphFilterLinesHoriz(arr, kernel_len = 51):\n",
    "#     kernel = np.ones((1,kernel_len),np.uint8)\n",
    "#     hlines = cv2.erode(arr, kernel, iterations = 1)\n",
    "#     hlines = cv2.dilate(hlines, kernel, iterations = 1)\n",
    "#     return hlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def morphFilterLinesVert(arr, kernel_len = 51, dilate_width = 1):\n",
    "#     vkernel = np.ones((kernel_len, 1), np.uint8)\n",
    "#     lines = cv2.erode(arr, vkernel, iterations = 1)\n",
    "#     lines = cv2.dilate(lines, vkernel, iterations = 1)\n",
    "#     if dilate_width > 1:\n",
    "#         hkernel = np.ones((1, dilate_width), np.uint8)\n",
    "#         lines = cv2.dilate(lines, hkernel, iterations = 1)\n",
    "#     return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphFilterRectangle(arr, kernel_height, kernel_width):\n",
    "    kernel = np.ones((kernel_height, kernel_width),np.uint8)\n",
    "    result = cv2.erode(arr, kernel, iterations = 1)\n",
    "    result = cv2.dilate(result, kernel, iterations = 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateStaffLines(arr, kernel_len, notebarfilt_len, notebar_removal):\n",
    "    lines = morphFilterRectangle(arr, 1, kernel_len) # isolate horizontal lines\n",
    "    notebarsOnly = morphFilterRectangle(lines, notebarfilt_len, 1) # isolate thick notebars\n",
    "    result = np.clip(lines - notebar_removal*notebarsOnly, 0, None) # subtract out notebars\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlines = isolateStaffLines(X2, morphFilterHorizLineSize, notebarFiltLen, notebarRemoval)\n",
    "showGrayscaleImage(hlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def computeStaveFeatureMap(arr, combfilts, rowfiltlen):\n",
    "#     featmap = []\n",
    "#     rowfilt = np.ones((1, rowfiltlen))\n",
    "#     for i in range(combfilts.shape[0]):\n",
    "#         m = convolve2d(arr, np.fliplr(np.flipud(combfilts[i])), mode = 'valid')\n",
    "#         m = convolve2d(m, rowfilt, mode = 'same')\n",
    "#         featmap.append(m)\n",
    "#     featmap = np.array(featmap)\n",
    "#     return featmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combfilts, stavelens = createCombFilters(targetLineSep, combFilterSizeTol)\n",
    "# featmap = computeStaveFeatureMap(hlines, combfilts, morphFilterHorizLineSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombFilter(lineSep):\n",
    "    # generate comb filter of specified length\n",
    "    # e.g. if length is 44, then spikes at indices 0, 11, 22, 33, 44\n",
    "    # e.g. if length is 43, then spikes at 0 [1.0], 10 [.25], 11 [.75], 21 [.5], 22 [.5], 32 [.75], 33 [.25], 43 [1.0]\n",
    "    stavelen = int(np.ceil(4 * lineSep)) + 1\n",
    "    combfilt = np.zeros(stavelen)\n",
    "    for i in range(5):\n",
    "        idx = i * lineSep\n",
    "        idx_below = int(idx)\n",
    "        idx_above = idx_below + 1\n",
    "        remainder = idx - idx_below\n",
    "        combfilt[idx_below] = 1 - remainder\n",
    "        if idx_above < stavelen:\n",
    "            combfilt[idx_above] = remainder\n",
    "    return combfilt, stavelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeStaveFeatureMap(img, ncols, lrange, urange, delta):\n",
    "    \n",
    "    # break image into columns, calculate row medians\n",
    "    imgHeight, imgWidth = img.shape\n",
    "    rowSums = np.zeros((imgHeight, ncols))\n",
    "    colWidth = int(np.ceil(imgWidth/ncols))\n",
    "    for i in range(ncols):\n",
    "        startCol = i * colWidth\n",
    "        endCol = min((i+1)*colWidth, imgWidth)\n",
    "        rowSums[:,i] = np.sum(img[:,startCol:endCol], axis=1)\n",
    "    \n",
    "    # apply comb filters\n",
    "    lineseps = np.arange(lrange, urange, delta)\n",
    "    maxFiltSize = int(np.ceil(4 * lineseps[-1])) + 1\n",
    "    featmap = np.zeros((len(lineseps), imgHeight - maxFiltSize + 1, ncols))\n",
    "    stavelens = np.zeros(len(lineseps), dtype=np.int)\n",
    "    for i, linesep in enumerate(lineseps):\n",
    "        filt, stavelen = getCombFilter(linesep)\n",
    "        padded = np.zeros((maxFiltSize, 1))\n",
    "        padded[0:len(filt),:] = filt.reshape((-1,1))\n",
    "        featmap[i,:,:] = convolve2d(rowSums, np.flipud(np.fliplr(padded)), mode = 'valid')\n",
    "        stavelens[i] = stavelen\n",
    "        \n",
    "    return featmap, stavelens, colWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featmap, stavelens, columnWidth = computeStaveFeatureMap(hlines, calcStaveFeatureMap_NumCols, calcStaveFeatureMap_LowerRange, calcStaveFeatureMap_UpperRange, calcStaveFeatureMap_Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notehead Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphFilterCircle(pimg, sz_reduce = 5, sz_expand = 0):\n",
    "    kernel_reduce = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (sz_reduce, sz_reduce))\n",
    "    result = cv2.dilate(np.array(pimg), kernel_reduce, iterations = 1)\n",
    "    if sz_expand > 0:\n",
    "        kernel_expand = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (sz_expand, sz_expand))\n",
    "        result = cv2.erode(result, kernel_expand, iterations = 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im3 = morphFilterCircle(pim2, morphFilterCircleSizeReduce, morphFilterCircleSizeExpand) # from here on use raw pixel values, not PIL image object\n",
    "showGrayscaleImage(im3, maxval = 255, inverted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectNoteheadBlobs(img, minarea, maxarea):\n",
    "    \n",
    "    # define blob detector\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "    # Change thresholds\n",
    "    # params.minThreshold = 100;\n",
    "    # params.maxThreshold = 200;\n",
    "\n",
    "    # Filter by Area\n",
    "    # params.filterByArea = True\n",
    "    params.minArea = minarea\n",
    "    params.maxArea = maxarea\n",
    "\n",
    "    # Filter by Circularity\n",
    "    # params.filterByCircularity = True\n",
    "    # params.minCircularity = 0.1\n",
    "\n",
    "    # Filter by Convexity\n",
    "    # params.filterByConvexity = True\n",
    "    # params.minConvexity = 0.87\n",
    "\n",
    "    # Filter by Inertia\n",
    "    # params.filterByInertia = True\n",
    "    # params.minInertiaRatio = 0.01\n",
    "\n",
    "    # Create a detector with the parameters\n",
    "    detector = cv2.SimpleBlobDetector_create(params)\n",
    "    \n",
    "    keypoints = detector.detect(img)\n",
    "    im_with_keypoints = cv2.drawKeypoints(np.array(img), keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return keypoints, im_with_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showColorImage(X, sz = (10,10)):\n",
    "    plt.figure(figsize = sz)\n",
    "    plt.imshow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints, im_with_keypoints = detectNoteheadBlobs(im3, notedetect_minarea, notedetect_maxarea)\n",
    "showColorImage(im_with_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteTemplate(arr, keypoints, sz = 21):\n",
    "    template = np.zeros((sz,sz))\n",
    "    L = (sz - 1)//2\n",
    "    #crops = []\n",
    "    numCrops = 0\n",
    "    for k in keypoints:\n",
    "        xloc = int(np.round(k.pt[0])) # col\n",
    "        yloc = int(np.round(k.pt[1])) # row\n",
    "        if xloc - L >= 0 and xloc + L + 1 <= arr.shape[1] and yloc - L >= 0 and yloc + L + 1 <= arr.shape[0]:\n",
    "            crop = arr[yloc-L:yloc+L+1,xloc-L:xloc+L+1]\n",
    "            #crops.append(crop)\n",
    "            template += crop\n",
    "            numCrops += 1\n",
    "    if numCrops > 0:\n",
    "        template = template / numCrops\n",
    "    #template = template - np.mean(template.ravel()) # will be used as a filter, so make zero mean\n",
    "    return template, numCrops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = getNormImage(im3) # im indicates grayscale [0, 255], X indicates [0, 1] inverted grayscale\n",
    "ntemplate, numCrops = getNoteTemplate(X3, keypoints, noteTemplateSize)\n",
    "showGrayscaleImage(ntemplate, (3,3), maxval = 1, inverted = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(np.sum(binarize_otsu(ntemplate)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getRandomCrops(arr, N=200, sz=21):\n",
    "#     crops = np.zeros((N,sz,sz))\n",
    "#     L = (sz-1)//2\n",
    "#     for i in range(N):\n",
    "#         yloc = np.random.randint(L, arr.shape[0]-L) # row\n",
    "#         xloc = np.random.randint(L, arr.shape[1]-L) # col\n",
    "#         crops[i,:,:] = arr[yloc-L:yloc+L+1,xloc-L:xloc+L+1]\n",
    "#     return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def comparePosNegDistributions(pcrops, ncrops, template):\n",
    "#     pos = []\n",
    "#     for crop in pcrops:\n",
    "#         pos.append(np.sum(crop * template))\n",
    "#     neg = []\n",
    "#     for crop in ncrops:\n",
    "#         neg.append(np.sum(crop*template))\n",
    "#     sns.kdeplot(np.array(pos))\n",
    "#     sns.kdeplot(np.array(neg))\n",
    "#     return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncrops = getRandomCrops(X3)\n",
    "# pos, neg = comparePosNegDistributions(pcrops, ncrops, ntemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptiveNoteheadDetect(arr, template, noteTolRatio, chordBlockSpecs):\n",
    "    #filtered = convolve2d(arr, np.flipud(np.fliplr(template)), mode='same', boundary='symm')\n",
    "    binarized, thresh = binarize_otsu(arr)\n",
    "    templateSpecs = getNoteTemplateSpecs(template, thresh)\n",
    "    labels = measure.label(binarized)\n",
    "    notes = []\n",
    "    for region in regionprops(labels):\n",
    "        if isValidNotehead(region, noteTolRatio, templateSpecs):\n",
    "            notes.append(region.bbox)\n",
    "        elif isValidChordBlock(region, chordBlockSpecs, templateSpecs):\n",
    "            chordNotes = extractNotesFromChordBlock(region, templateSpecs)\n",
    "            notes.extend(chordNotes)\n",
    "    return notes, binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_otsu(img):\n",
    "    arr = np.array(img)\n",
    "    thresh = filters.threshold_otsu(arr)\n",
    "    binarized = arr > thresh\n",
    "    return binarized, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteTemplateSpecs(template, thresh):\n",
    "    binarized = template > thresh\n",
    "    labels = measure.label(binarized)\n",
    "    maxH, maxW, maxArea = (0, 0, 0)\n",
    "    for region in regionprops(labels):\n",
    "        curH = region.bbox[2] - region.bbox[0]\n",
    "        curW = region.bbox[3] - region.bbox[1]\n",
    "        curArea = region.area\n",
    "        if curArea > maxArea:\n",
    "            maxArea = curArea\n",
    "            maxH = curH\n",
    "            maxW = curW\n",
    "    return (maxH, maxW, maxArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidNotehead(region, tol_ratio, templateSpecs):\n",
    "    templateH, templateW, templateArea = templateSpecs\n",
    "    max_ratio = 1 + tol_ratio\n",
    "    min_ratio = 1 / (1 + tol_ratio)\n",
    "    curH = region.bbox[2] - region.bbox[0]\n",
    "    curW = region.bbox[3] - region.bbox[1]\n",
    "    curArea = region.area\n",
    "    curRatio = 1.0 * curH / curW\n",
    "    templateRatio = 1.0 * templateH / templateW\n",
    "    validH = curH < templateH * max_ratio and curH > templateH * min_ratio\n",
    "    validW = curW < templateW * max_ratio and curW > templateW * min_ratio\n",
    "    validArea = curArea < templateArea * max_ratio * max_ratio and curArea > templateArea * min_ratio * min_ratio\n",
    "    validRatio = curRatio < templateRatio * max_ratio and curRatio > templateRatio * min_ratio\n",
    "    result = validH and validW and validRatio and validArea\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidChordBlock(region, params, templateSpecs):\n",
    "    templateH, templateW, templateArea = templateSpecs\n",
    "    minH, maxH, minW, maxW, minArea, maxArea, minNotes, maxNotes = params\n",
    "    curH = region.bbox[2] - region.bbox[0]\n",
    "    curW = region.bbox[3] - region.bbox[1]\n",
    "    curArea = region.area\n",
    "    curNotes = int(np.round(curArea / templateArea))\n",
    "    validH = curH >= minH * templateH and curH <= maxH * templateH\n",
    "    validW = curW >= minW * templateW and curW <= maxW * templateW\n",
    "    validArea = curArea >= minArea * templateArea and curArea <= maxArea * templateArea\n",
    "    validNotes = curNotes >= minNotes and curNotes <= maxNotes\n",
    "    result = validH and validW and validArea and validNotes\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNotesFromChordBlock(region, templateSpecs):\n",
    "    # use kmeans to estimate note centers\n",
    "    templateH, templateW, templateArea = templateSpecs\n",
    "    numNotes = int(np.round(region.area / templateArea))\n",
    "    regionCoords = np.array(region.coords)\n",
    "    kmeans = KMeans(n_clusters=numNotes, n_init = 1).fit(regionCoords)\n",
    "    bboxes = []\n",
    "    for (r,c) in kmeans.cluster_centers_:\n",
    "        rmin = int(np.round(r - templateH/2))\n",
    "        rmax = int(np.round(r + templateH/2))\n",
    "        cmin = int(np.round(c - templateW/2))\n",
    "        cmax = int(np.round(c + templateW/2))\n",
    "        bboxes.append((rmin, cmin, rmax, cmax))\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeLabels(img, bboxes):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for (minr, minc, maxr, maxc) in bboxes:\n",
    "        rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordBlockSpecs = (chordBlock_minH, chordBlock_maxH, chordBlock_minW, chordBlock_maxW, chordBlock_minArea, chordBlock_maxArea, chordBlock_minNotes, chordBlock_maxNotes)\n",
    "notes, img_binarized_notes = adaptiveNoteheadDetect(X3, ntemplate, notedetect_tol_ratio, chordBlockSpecs)\n",
    "#showGrayscaleImage(img_binarized_notes)\n",
    "visualizeLabels(img_binarized_notes, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showGrayscaleImage(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getContextRegion(arr, center, deltaR = 10, deltaC = 10):\n",
    "#     r = int(np.round(center[0]))\n",
    "#     c = int(np.round(center[1]))\n",
    "#     rupper = min(r + deltaR + 1, arr.shape[0])\n",
    "#     rlower = max(r - deltaR, 0)\n",
    "#     cupper = min(c + deltaC + 1, arr.shape[1])\n",
    "#     clower = max(c - deltaC, 0)\n",
    "#     crop = arr[rlower:rupper,clower:cupper] \n",
    "#     return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getContextRegion2(arr, rstart, rend, cstart, cend):\n",
    "#     rupper = min(rend, arr.shape[0])\n",
    "#     rlower = max(rstart, 0)\n",
    "#     cupper = min(cend, arr.shape[1])\n",
    "#     clower = max(cstart, 0)\n",
    "#     crop = arr[rlower:rupper,clower:cupper] \n",
    "#     return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteheadInfo(bboxes):\n",
    "    nhlocs = [(.5*(bbox[0] + bbox[2]), .5*(bbox[1] + bbox[3])) for bbox in bboxes]\n",
    "    nhlens = [(bbox[2] - bbox[0]) for bbox in bboxes]\n",
    "    nhwidths = [(bbox[3] - bbox[1]) for bbox in bboxes]\n",
    "    nhlen_est = int(np.ceil(np.mean(nhlens)))\n",
    "    nhwidth_est = int(np.ceil(np.mean(nhwidths)))\n",
    "    return nhlocs, nhlen_est, nhwidth_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhlocs, nhlen_est, nhwidth_est = getNoteheadInfo(notes)\n",
    "nhlen_est, nhwidth_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Note Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEstStaffLineLocs(featmap, nhlocs, stavelens, colWidth, deltaRowMax, globalOffset = 0):\n",
    "    preds = []\n",
    "    if np.isscalar(globalOffset):\n",
    "        globalOffset = [globalOffset] * len(nhlocs)\n",
    "    for i, nhloc in enumerate(nhlocs):\n",
    "        r = int(np.round(nhloc[0]))\n",
    "        c = int(np.round(nhloc[1]))\n",
    "        rupper = min(r + deltaRowMax + 1 + globalOffset[i], featmap.shape[1])\n",
    "        rlower = max(r - deltaRowMax + globalOffset[i], 0)\n",
    "        featmapIdx = c // colWidth\n",
    "#         if featmapIdx == 0:\n",
    "#             neighborIdx1 = 1\n",
    "#             neighborIdx2 = 2\n",
    "#         elif featmapIdx == featmap.shape[2] - 1:\n",
    "#             neighborIdx1 = featmap.shape[2] - 3\n",
    "#             neighborIdx2 = featmap.shape[2] - 2\n",
    "#         else:\n",
    "#             neighborIdx1 = featmapIdx - 1\n",
    "#             neighborIdx2 = featmapIdx + 1\n",
    "        regCurrent = np.squeeze(featmap[:, rlower:rupper, featmapIdx])\n",
    "        #regNeighbor1 = np.squeeze(featmap[:, rlower:rupper, neighborIdx1])\n",
    "        #regNeighbor2 = np.squeeze(featmap[:, rlower:rupper, neighborIdx2])\n",
    "        mapidx, roffset = np.unravel_index(regCurrent.argmax(), regCurrent.shape)\n",
    "        #mapidx_current, roffset_current = np.unravel_index(regCurrent.argmax(), regCurrent.shape)\n",
    "        #mapidx_neighbor1, roffset_neighbor1 = np.unravel_index(regNeighbor1.argmax(), regNeighbor1.shape)\n",
    "        #mapidx_neighbor2, roffset_neighbor2 = np.unravel_index(regNeighbor2.argmax(), regNeighbor2.shape)\n",
    "        #mapidx = int(np.median([mapidx_current, mapidx_neighbor1, mapidx_neighbor2]))\n",
    "        #roffset = np.median([roffset_current, roffset_neighbor1, roffset_neighbor2])\n",
    "    \n",
    "        rstart = rlower + roffset\n",
    "        rend = rstart + stavelens[mapidx] - 1\n",
    "        preds.append((rstart, rend, c, r, mapidx))\n",
    "        \n",
    "    sfiltlen = int(np.round(np.median([stavelens[tup[4]] for tup in preds])))\n",
    "    return preds, sfiltlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeEstStaffLines(preds, arr):\n",
    "    showGrayscaleImage(arr, (15,15))\n",
    "    rows1 = np.array([pred[0] for pred in preds]) # top staff line\n",
    "    rows2 = np.array([pred[1] for pred in preds]) # bottom staff line\n",
    "    cols = np.array([pred[2] for pred in preds]) # nh col\n",
    "    rows3 = np.array([pred[3] for pred in preds]) # nh row\n",
    "    plt.scatter(cols, rows1, c = 'r', s = 3)\n",
    "    plt.scatter(cols, rows2, c = 'b', s = 3)\n",
    "    plt.scatter(cols, rows3, c = 'y', s = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowInitial, int(-2*targetLineSep))\n",
    "visualizeEstStaffLines(estStaffLineLocs, hlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateStaffMidpoints(preds, clustersMin, clustersMax, threshold):\n",
    "    r = np.array([.5*(tup[0] + tup[1]) for tup in preds]) # midpts of estimated stave locations\n",
    "    models = []\n",
    "    for numClusters in range(clustersMin, clustersMax + 1):\n",
    "        kmeans = KMeans(n_clusters=numClusters, n_init=1, random_state = 0).fit(r.reshape(-1,1))\n",
    "        sorted_list = np.array(sorted(np.squeeze(kmeans.cluster_centers_)))\n",
    "        mindiff = np.min(sorted_list[1:] - sorted_list[0:-1])\n",
    "        if mindiff < threshold:\n",
    "            break\n",
    "        models.append(kmeans)\n",
    "    staffMidpts = np.sort(np.squeeze(models[-1].cluster_centers_))\n",
    "    return staffMidpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugStaffMidpointClustering(preds):\n",
    "    r = np.array([.5*(tup[0] + tup[1]) for tup in preds]) # midpts of estimated stave locations\n",
    "    inertias = []\n",
    "    mindiffs = []\n",
    "    clusterRange = np.arange(2,12)\n",
    "    for numClusters in clusterRange:\n",
    "        kmeans = KMeans(n_clusters=numClusters, n_init=1).fit(r.reshape(-1,1))\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        sorted_list = np.array(sorted(np.squeeze(kmeans.cluster_centers_)))\n",
    "        diffs = sorted_list[1:] - sorted_list[0:-1]\n",
    "        mindiffs.append(np.min(diffs))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(clusterRange, np.log(inertias))\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(clusterRange, mindiffs)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Min Centroid Separation')\n",
    "    plt.axhline(60, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeStaffMidpointClustering(preds, centers):\n",
    "    r = np.array([.5*(tup[0] + tup[1]) for tup in preds]) # midpts of estimated stave locations\n",
    "    plt.plot(r, np.random.uniform(size = len(r)), '.')\n",
    "    for center in centers:\n",
    "        plt.axvline(x=center, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staveMidpts = estimateStaffMidpoints(estStaffLineLocs, minNumStaves, maxNumStaves, minStaveSeparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugStaffMidpointClustering(estStaffLineLocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeStaffMidpointClustering(estStaffLineLocs, staveMidpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignNoteheadsToStaves(nhlocs, staveCenters):\n",
    "    nhrows = np.matlib.repmat([tup[0] for tup in nhlocs], len(staveCenters), 1)\n",
    "    centers = np.matlib.repmat(staveCenters.reshape((-1,1)), 1, len(nhlocs))\n",
    "    staveIdxs = np.argmin(np.abs(nhrows - centers), axis=0)\n",
    "    offsets = staveCenters[staveIdxs] - nhrows[0,:] # row offset between note and staff midpoint\n",
    "    return staveIdxs, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeClusters(arr, nhlocs, clusters):\n",
    "    showGrayscaleImage(arr)\n",
    "    rows = np.array([tup[0] for tup in nhlocs])\n",
    "    cols = np.array([tup[1] for tup in nhlocs])\n",
    "    plt.scatter(cols, rows, c=clusters)\n",
    "    for i in range(len(clusters)):\n",
    "        plt.text(cols[i], rows[i] - 15, str(clusters[i]), fontsize = 12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staveIdxs, nhRowOffsets = assignNoteheadsToStaves(nhlocs, staveMidpts)\n",
    "visualizeClusters(X2, nhlocs, staveIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowRefined, (nhRowOffsets - 2*targetLineSep).astype(np.int))\n",
    "visualizeEstStaffLines(estStaffLineLocs, hlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getStaveTemplate(arr, preds, filtlen, rbuff = 0):\n",
    "#     crops = np.zeros((len(preds), 2*rbuff + filtlen, 1))\n",
    "#     for i, tup in enumerate(preds):\n",
    "#         (rstart, rend, c, r, filtidx) = tup\n",
    "#         reg = getContextRegion2(arr, rstart - rbuff, rstart + filtlen + rbuff, c, c + 1)\n",
    "#         crops[i, 0:reg.shape[0], :] = reg\n",
    "#     template = np.mean(crops, axis=0)\n",
    "#     template = template - np.mean(template.ravel())\n",
    "#     return template, crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemplate, scrops = getStaveTemplate(hlines, estStaffLineLocs, sfiltlen)\n",
    "# showGrayscaleImage(stemplate)\n",
    "# sfiltlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateNoteLabels(preds):\n",
    "    nhvals = [] # estimated note labels\n",
    "    for i, (rstart, rend, c, r, filtidx) in enumerate(preds):       \n",
    "        # if a stave has height L, there are 8 stave locations in (L-1) pixel rows\n",
    "        staveMidpt = .5 * (rstart + rend)\n",
    "        noteStaveLoc = -1.0 * (r - staveMidpt) * 8 / (rend - rstart)\n",
    "        nhval = int(np.round(noteStaveLoc))\n",
    "        nhvals.append(nhval)\n",
    "    return nhvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeNoteLabels(arr, vals, locs):\n",
    "    showGrayscaleImage(arr)\n",
    "    rows = np.array([loc[0] for loc in locs])\n",
    "    cols = np.array([loc[1] for loc in locs])\n",
    "    plt.scatter(cols, rows, color='blue')\n",
    "    for i in range(len(rows)):\n",
    "        plt.text(cols[i], rows[i] - 15, str(vals[i]), fontsize = 12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhvals = estimateNoteLabels(estStaffLineLocs)\n",
    "visualizeNoteLabels(X2, nhvals, nhlocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster staves & noteheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateBarlines(im, morphFilterVertLineLength, morphFilterVertLineWidth):\n",
    "    hkernel = np.ones((1, morphFilterVertLineWidth), np.uint8) # dilate first to catch warped barlines\n",
    "    vlines = cv2.dilate(im, hkernel, iterations = 1)\n",
    "    vlines = morphFilterRectangle(vlines, morphFilterVertLineLength, 1) # then filter for tall vertical lines\n",
    "    return vlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlines = isolateBarlines(X2, morphFilterVertLineLength, morphFilterVertLineWidth)\n",
    "showGrayscaleImage(vlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineStaveGrouping(staveMidpts, vlines):\n",
    "    \n",
    "    N = len(staveMidpts)\n",
    "    rowSums = np.sum(vlines, axis=1)\n",
    "    \n",
    "    # grouping A: 0-1, 2-3, 4-5, ...\n",
    "    elems_A = []\n",
    "    map_A = {}\n",
    "    for i, staveIdx in enumerate(np.arange(0, N, 2)):\n",
    "        if staveIdx+1 < N:\n",
    "            startRow = int(staveMidpts[staveIdx])\n",
    "            endRow = int(staveMidpts[staveIdx+1]) + 1\n",
    "            elems_A.extend(rowSums[startRow:endRow])\n",
    "            map_A[staveIdx] = staveIdx\n",
    "            map_A[staveIdx+1] = staveIdx + 1\n",
    "        else:\n",
    "            map_A[staveIdx] = -1 # unpaired stave\n",
    "    \n",
    "    # grouping B: 1-2, 3-4, 5-6, ...\n",
    "    elems_B = []\n",
    "    map_B = {}\n",
    "    map_B[0] = -1 \n",
    "    for i, staveIdx in enumerate(np.arange(1, N, 2)):\n",
    "        if staveIdx+1 < N:\n",
    "            startRow = int(staveMidpts[staveIdx])\n",
    "            endRow = int(staveMidpts[staveIdx+1]) + 1\n",
    "            elems_B.extend(rowSums[startRow:endRow])\n",
    "            map_B[staveIdx] = staveIdx - 1\n",
    "            map_B[staveIdx + 1] = staveIdx\n",
    "        else:\n",
    "            map_B[staveIdx] = -1\n",
    "    \n",
    "    evidence_A = np.median(elems_A)\n",
    "    evidence_B = np.median(elems_B)\n",
    "    if evidence_A > evidence_B:\n",
    "        mapping = map_A\n",
    "    else:\n",
    "        mapping = map_B\n",
    "    \n",
    "    return mapping, (evidence_A, evidence_B, elems_A, elems_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staveMapping, evidence = determineStaveGrouping(staveMidpts, vlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.median(evidence[2]), np.median(evidence[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugStaveGrouping(vlines, staveCenters):\n",
    "    plt.plot(np.sum(vlines, axis=1))\n",
    "    for m in staveCenters:\n",
    "        plt.axvline(m, color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugStaveGrouping(vlines, staveMidpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def estimateBarlineThreshold(heights, maxVal):\n",
    "#     pruned = np.array([h for h in heights if h < maxVal]) # toss out outliers (probably shadows)\n",
    "#     if len(pruned) > 1:\n",
    "#         thresh = filters.threshold_otsu(np.expand_dims(pruned, axis=0))\n",
    "#     else:\n",
    "#         thresh = 0\n",
    "#     return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isValidBarline(region, minBarlineLen, maxBarlineLen, maxBarlineWidth):\n",
    "#     height = region.bbox[2] - region.bbox[0]\n",
    "#     width = region.bbox[3] - region.bbox[1]\n",
    "#     aboveMin = height > minBarlineLen\n",
    "#     belowMaxLen = height < maxBarlineLen\n",
    "#     belowMaxWidth = width < maxBarlineWidth\n",
    "#     result = aboveMin and belowMaxLen and belowMaxWidth\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def barlineDetect(arr, maxBarlineLen = 200, maxBarlineWidth = 10):\n",
    "#     binarized, _ = binarize_otsu(arr)\n",
    "#     labels = measure.label(binarized)\n",
    "#     heights = [region.bbox[2] - region.bbox[0] for region in regionprops(labels)]\n",
    "#     minBarlineLen = estimateBarlineThreshold(heights, maxBarlineLen)\n",
    "#     bars = []\n",
    "#     for region in regionprops(labels):\n",
    "#         if isValidBarline(region, minBarlineLen, maxBarlineLen, maxBarlineWidth):\n",
    "#             bars.append(region.bbox)\n",
    "#     return bars, binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxBarlineLen = int(vlines.shape[0] * maxBarlineLenFactor)\n",
    "# bars, img_binarized_bars = barlineDetect(vlines, maxBarlineLen, maxBarlineWidth)\n",
    "# visualizeLabels(img_binarized_bars, bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calcOverlap(seg1, seg2):\n",
    "#     lb = max(seg1[0], seg2[0])\n",
    "#     ub = min(seg1[1], seg2[1])\n",
    "#     overlap = np.clip(ub - lb, 0, None)\n",
    "#     return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clusterBarlines(bars):\n",
    "    \n",
    "#     # cluster overlapping barlines\n",
    "#     clusters = -1*np.ones(len(bars), dtype=np.int8)\n",
    "#     clusterIndex = 0\n",
    "#     for i in range(len(bars)):\n",
    "#         if clusters[i] == -1: # has not been assigned a cluster yet\n",
    "#             anchor = (bars[i][0], bars[i][2])\n",
    "#             for j in range(i,len(bars)):\n",
    "#                 other = (bars[j][0], bars[j][2])\n",
    "#                 overlap = calcOverlap(anchor, other)\n",
    "#                 if overlap > 0:\n",
    "#                     clusters[j] = clusterIndex\n",
    "#             clusterIndex += 1\n",
    "    \n",
    "#     # determine bounds of each cluster\n",
    "#     cluster_bnds = []\n",
    "#     for i in range(clusterIndex):\n",
    "#         selected = np.array([(reg[0], reg[2]) for j, reg in enumerate(bars) if clusters[j] == i])\n",
    "#         lb = np.min(selected)\n",
    "#         ub = np.max(selected)\n",
    "#         cluster_bnds.append((lb, ub))\n",
    "    \n",
    "#     return clusters, cluster_bnds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualizeBarClusters(arr, clusters):\n",
    "#     showGrayscaleImage(arr, (5,5))\n",
    "#     ax = plt.gca()\n",
    "#     minc = 0\n",
    "#     maxc = arr.shape[1] - 1\n",
    "#     for (minr, maxr) in clusters:\n",
    "#         rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr, fill=False, edgecolor='red', linewidth=2)\n",
    "#         ax.add_patch(rect)\n",
    "#     ax.set_axis_off()\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barline_clusters, barline_clusterbnds = clusterBarlines(bars)\n",
    "# visualizeBarClusters(vlines, barline_clusterbnds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterNoteheads(staveIdxs, mapping):\n",
    "    clusterIdxs = [mapping[staveIdx] for staveIdx in staveIdxs]\n",
    "    maxClusterIdx = np.max(np.array(clusterIdxs))\n",
    "    clusterPairs = []\n",
    "    for i in range(0, maxClusterIdx, 2):\n",
    "        clusterPairs.append((i,i+1))\n",
    "    return clusterIdxs, clusterPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhclusters, clusterPairs = clusterNoteheads(staveIdxs, staveMapping)\n",
    "visualizeClusters(X2, nhlocs, nhclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clusterNoteheads(midpts, barlines):\n",
    "#     clusters = np.zeros(len(midpts), dtype=np.int8) # cluster index of each note\n",
    "#     for i, midpt in enumerate(midpts):\n",
    "#         clusteridx = -1 # -1 means notehead is not within a valid barline region\n",
    "#         for j, (lb, ub) in enumerate(barlines):\n",
    "#             if midpt >= lb and midpt <= ub:\n",
    "#                 lb_dist = np.abs(midpt - lb)\n",
    "#                 ub_dist = np.abs(midpt - ub)\n",
    "#                 if lb_dist < ub_dist: # rh staff\n",
    "#                     clusteridx = 2*j\n",
    "#                 else:\n",
    "#                     clusteridx = 2*j + 1 # lh staff\n",
    "#                 break\n",
    "#         clusters[i] = clusteridx     \n",
    "#     return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getClusterPairs(nhclusters, numLines):\n",
    "#     # discard any \"lines\" that have no notes (may be a shadow)\n",
    "#     validClusterIdxs = set(nhclusters)    \n",
    "#     result = []\n",
    "#     for i in range(0, 2*numLines, 2):\n",
    "#         if i in validClusterIdxs or i+1 in validClusterIdxs:\n",
    "#             result.append((i,i+1))\n",
    "#     result = np.array(result)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nhclusters = clusterNoteheads(nhmidpts, barline_clusterbnds)\n",
    "# clusterPairs = getClusterPairs(nhclusters, len(barline_clusterbnds))\n",
    "# visualizeClusters(X2, estStaffLineLocs, nhclusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Bootleg Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSingleBootlegLine(nhdata, clusterR, clusterL, minColDiff, repeatNotes = 1, filler = 1):\n",
    "    notes = [tup for tup in nhdata if tup[3] == clusterR or tup[3] == clusterL]\n",
    "    notes = sorted(notes, key = lambda tup: (tup[1], tup[0])) # sort by column, then row\n",
    "    collapsed = collapseSimultaneousEvents(notes, minColDiff) # list of (rows, cols, vals, clusters)\n",
    "    bscore, eventIndices, staffLinesBoth, _, _ = constructBootlegScore(collapsed, clusterR, clusterL, repeatNotes, filler)\n",
    "    return bscore, collapsed, eventIndices, staffLinesBoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapseSimultaneousEvents(notes, minColDiff):\n",
    "    assigned = np.zeros(len(notes), dtype=bool)\n",
    "    events = [] # list of simultaneous note events\n",
    "    for i, (row, col, val, cluster) in enumerate(notes):\n",
    "        if assigned[i]: # has already been assigned\n",
    "            continue\n",
    "        rows = [row] # new event\n",
    "        cols = [col]\n",
    "        vals = [val]\n",
    "        clusters = [cluster]\n",
    "        assigned[i] = True\n",
    "        for j in range(i+1, len(notes)):\n",
    "            nrow, ncol, nval, ncluster = notes[j]\n",
    "            if ncol - col < minColDiff: # assign to same event if close\n",
    "                rows.append(nrow)\n",
    "                cols.append(ncol)\n",
    "                vals.append(nval)\n",
    "                clusters.append(ncluster)\n",
    "                assigned[j] = True\n",
    "            else:\n",
    "                break\n",
    "        events.append((rows, cols, vals, clusters))\n",
    "    \n",
    "    assert(np.all(assigned))\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructBootlegScore(noteEvents, clusterIndexRH, clusterIndexLH, repeatNotes = 1, filler = 1):\n",
    "    # note that this has to match generateBootlegScore() in the previous notebook!\n",
    "    rh_dim = 34 # E3 to C8 (inclusive)\n",
    "    lh_dim = 28 # A1 to G4 (inclusive)\n",
    "    rh = [] # list of arrays of size rh_dim\n",
    "    lh = [] # list of arrays of size lh_dim\n",
    "    eventIndices = [] # index of corresponding simultaneous note event\n",
    "    for i, (rows, cols, vals, clusters) in enumerate(noteEvents):\n",
    "        \n",
    "        # insert empty filler columns between note events\n",
    "        if i > 0:\n",
    "            for j in range(filler):\n",
    "                rh.append(np.zeros((rh_dim,1)))\n",
    "                lh.append(np.zeros((lh_dim,1)))\n",
    "                eventIndices.append(i-1) # assign filler to previous event\n",
    "\n",
    "        # insert note events columns\n",
    "        rhvec, lhvec = getNoteheadPlacement(vals, clusters, rh_dim, lh_dim, clusterIndexRH, clusterIndexLH)\n",
    "        for j in range(repeatNotes):\n",
    "            rh.append(rhvec)\n",
    "            lh.append(lhvec)\n",
    "            eventIndices.append(i)\n",
    "    rh = np.squeeze(np.array(rh)).reshape((-1, rh_dim)).T # reshape handles case when len(rh) == 1\n",
    "    lh = np.squeeze(np.array(lh)).reshape((-1, lh_dim)).T\n",
    "    both = np.vstack((lh, rh))\n",
    "    staffLinesRH = [7,9,11,13,15]\n",
    "    staffLinesLH = [13,15,17,19,21]\n",
    "    staffLinesBoth = [13,15,17,19,21,35,37,39,41,43]\n",
    "    return both, eventIndices, staffLinesBoth, (rh, staffLinesRH), (lh, staffLinesLH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoteheadPlacement(vals, clusters, rdim, ldim, clusterRH, clusterLH):\n",
    "    rhvec = np.zeros((rdim, 1))\n",
    "    lhvec = np.zeros((ldim, 1))\n",
    "    assert(clusterLH == clusterRH + 1)\n",
    "    for (val, cluster) in zip(vals, clusters):\n",
    "        if cluster == clusterRH:\n",
    "            idx = val + 11\n",
    "            if idx >= 0 and idx < rdim:\n",
    "                rhvec[idx, 0] = 1\n",
    "        elif cluster == clusterLH:\n",
    "            idx = val + 17\n",
    "            if idx >= 0 and idx < ldim:\n",
    "                lhvec[idx, 0] = 1\n",
    "        else:\n",
    "            print(\"Invalid cluster: {} (LH {}, RH {})\".format(cluster, clusterLH, clusterRH))\n",
    "            sys.exit(1)\n",
    "    return rhvec, lhvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeBootlegScore(bs, lines):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(1 - bs, cmap = 'gray', origin = 'lower')\n",
    "    for l in range(1, bs.shape[0], 2):\n",
    "        plt.axhline(l, c = 'grey')\n",
    "    for l in lines:\n",
    "        plt.axhline(l, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of (row, col, value, cluster) tuples\n",
    "nhdata = [(int(np.round(nhlocs[i][0])), int(np.round(nhlocs[i][1])), nhvals[i], nhclusters[i]) for i in range(len(nhlocs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bscore, events, eventIndices, staffLinesBoth = generateSingleBootlegLine(nhdata, clusterR = 0, clusterL = 1, minColDiff = nhwidth_est, repeatNotes = 1, filler = 1)\n",
    "#visualizeBootlegScore(bscore, staffLinesBoth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQueryBootlegScore(nhdata, pairings, repeatNotes = 1, filler = 1, minColDiff = 10):\n",
    "    allScores = []\n",
    "    allEvents = []\n",
    "    globIndices = []\n",
    "    eventCount = 0\n",
    "    for i, (clusterR, clusterL) in enumerate(pairings):\n",
    "        score, events, eventIndices, staffLinesBoth = generateSingleBootlegLine(nhdata, clusterR, clusterL, minColDiff, repeatNotes, filler)\n",
    "        allScores.append(score)\n",
    "        allEvents.extend(events)\n",
    "        globIndices.extend([idx + eventCount for idx in eventIndices])\n",
    "        if i < len(pairings) - 1:\n",
    "            allScores.append(np.zeros((score.shape[0], filler))) # append filler columns between bootleg scores\n",
    "            globIndices.extend([globIndices[-1]] * filler) # map filler columns to last event index\n",
    "        eventCount += len(events)\n",
    "    panorama = np.hstack(allScores)\n",
    "    return panorama, allEvents, globIndices, staffLinesBoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeLongBootlegScore(bs, lines, chunksz = 150):\n",
    "    chunks = bs.shape[1] // chunksz + 1\n",
    "    for i in range(chunks):\n",
    "        startcol = i * chunksz\n",
    "        endcol = min((i + 1)*chunksz, bs.shape[1])\n",
    "        visualizeBootlegScore(bs[:,startcol:endcol], lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscore_query, events, eventIndices, staffLinesBoth = generateQueryBootlegScore(nhdata, clusterPairs, bootlegRepeatNotes, bootlegFiller, minColDiff = nhwidth_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLongBootlegScore(bscore_query, staffLinesBoth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align bootleg scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMidiBootlegScore(pkl_file):\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    bscore = d['bscore']\n",
    "    miditimes = d['times']\n",
    "    num_notes = np.array(d['num_notes'])\n",
    "    stafflines = d['stafflines']\n",
    "    return bscore, miditimes, num_notes, stafflines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costMetric(X,Y):\n",
    "    cost = -1 * np.dot(X,Y)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizedCostMetric(Q, R, numQueryNotes, numRefNotes):\n",
    "    cost = -1 * np.matmul(Q.T, R)\n",
    "    query_norm_factor = repmat(numQueryNotes.reshape((-1,1)), 1, R.shape[1])\n",
    "    ref_norm_factor = repmat(numRefNotes.reshape((1,-1)), Q.shape[1], 1)\n",
    "    norm_factor = np.maximum(query_norm_factor, ref_norm_factor) + 1e-8 # avoid divide by 0\n",
    "    norm_cost = cost / norm_factor\n",
    "    return norm_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignBootlegScores(query, ref, numRefNotes, steps = [1,1,1,2,2,1], weights = [1,1,2], optimized=True):\n",
    "    if optimized: # Cython implementation\n",
    "        # set params\n",
    "        assert len(steps) % 2 == 0, \"The length of steps must be even.\"\n",
    "        dn = np.array(steps[::2], dtype=np.uint32)\n",
    "        dm = np.array(steps[1::2], dtype=np.uint32)\n",
    "        dw = weights\n",
    "        subsequence = True\n",
    "        parameter = {'dn': dn, 'dm': dm, 'dw': dw, 'SubSequence': subsequence}\n",
    "\n",
    "        # Compute cost matrix\n",
    "        #cost = costMetric(query.T, ref)\n",
    "        numQueryNotes = np.sum(query, axis=0)\n",
    "        cost = normalizedCostMetric(query, ref, numQueryNotes, numRefNotes)\n",
    "\n",
    "        # DTW\n",
    "        [D, s] = DTW_Cost_To_AccumCostAndSteps(cost, parameter)\n",
    "        [wp, endCol, endCost] = DTW_GetPath(D, s, parameter)\n",
    "\n",
    "        # Reformat the output\n",
    "        wp = wp.T[::-1]\n",
    "    else: # librosa implementation\n",
    "        steps = np.array(steps).reshape((-1,2))\n",
    "        D, wp = lb.sequence.dtw(query, ref, step_sizes_sigma = steps, weights_mul = weights, subseq = True, metric = costMetric)\n",
    "    return D, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "DTYPE_INT32 = np.int32\n",
    "ctypedef np.int32_t DTYPE_INT32_t\n",
    "\n",
    "DTYPE_FLOAT = np.float64\n",
    "ctypedef np.float64_t DTYPE_FLOAT_t\n",
    "\n",
    "cdef DTYPE_FLOAT_t MAX_FLOAT = float('inf')\n",
    "\n",
    "# careful, without bounds checking can mess up memory - also can't use negative indices I think (like x[-1])\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def DTW_Cost_To_AccumCostAndSteps(Cin, parameter):\n",
    "    '''\n",
    "    Inputs\n",
    "        C: The cost Matrix\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "    Section for checking and catching errors in the inputs\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] C\n",
    "    try:\n",
    "        C = np.array(Cin, dtype=DTYPE_FLOAT)\n",
    "    except TypeError:\n",
    "        print(bcolors.FAIL + \"FAILURE: The type of the cost matrix is wrong - please pass in a 2-d numpy array\" + bcolors.ENDC)\n",
    "        return [-1, -1, -1]\n",
    "    except ValueError:\n",
    "        print(bcolors.FAIL + \"FAILURE: The type of the elements in the cost matrix is wrong - please have each element be a float (perhaps you passed in a matrix of ints?)\" + bcolors.ENDC)\n",
    "        return [-1, -1, -1]\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] dn\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] dm\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=1] dw\n",
    "    # make sure dn, dm, and dw are setup\n",
    "    # dn loading and exception handling\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        try:\n",
    "\n",
    "            dn = np.array(parameter['dn'], dtype=np.uint32)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dn (row steps) is wrong - please pass in a 1-d numpy array that holds uint32s\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"The type of the elements in dn (row steps) is wrong - please have each element be a uint32 (perhaps you passed a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.uint32)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=np.uint32)\n",
    "    # dm loading and exception handling\n",
    "    if 'dm'  in parameter.keys():\n",
    "        try:\n",
    "            dm = np.array(parameter['dm'], dtype=np.uint32)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dm (col steps) is wrong - please pass in a 1-d numpy array that holds uint32s\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of the elements in dm (col steps) is wrong - please have each element be a uint32 (perhaps you passed a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.uint32)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        print(bcolors.FAIL + \"dm (col steps) was not passed in (gave default value [1,0,1]) \" + bcolors.ENDC)\n",
    "        dm = np.array([1, 0, 1], dtype=np.uint32)\n",
    "    # dw loading and exception handling\n",
    "    if 'dw'  in parameter.keys():\n",
    "        try:\n",
    "            dw = np.array(parameter['dw'], dtype=DTYPE_FLOAT)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dw (step weights) is wrong - please pass in a 1-d numpy array that holds floats\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"FAILURE:The type of the elements in dw (step weights) is wrong - please have each element be a float (perhaps you passed ints or a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.float64)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        dw = np.array([1, 1, 1], dtype=DTYPE_FLOAT)\n",
    "        print(bcolors.FAIL + \"dw (step weights) was not passed in (gave default value [1,1,1]) \" + bcolors.ENDC)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Section where types are given to the variables we're going to use \n",
    "    '''\n",
    "    # create matrices to store our results (D and E)\n",
    "    cdef DTYPE_INT32_t numRows = C.shape[0] # only works with np arrays, use np.shape(x) will work on lists? want to force to use np though?\n",
    "    cdef DTYPE_INT32_t numCols = C.shape[1]\n",
    "    cdef DTYPE_INT32_t numDifSteps = np.size(dw)\n",
    "\n",
    "    cdef unsigned int maxRowStep = max(dn)\n",
    "    cdef unsigned int maxColStep = max(dm)\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=2] steps = np.zeros((numRows,numCols), dtype=np.uint32)\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost = np.ones((maxRowStep + numRows, maxColStep + numCols), dtype=DTYPE_FLOAT) * MAX_FLOAT\n",
    "\n",
    "    cdef DTYPE_FLOAT_t bestCost\n",
    "    cdef DTYPE_INT32_t bestCostIndex\n",
    "    cdef DTYPE_FLOAT_t costForStep\n",
    "    cdef unsigned int row, col\n",
    "    cdef unsigned int stepIndex\n",
    "\n",
    "    '''\n",
    "    The start of the actual algorithm, now that all our variables are set up\n",
    "    '''\n",
    "    # initializing the cost matrix - depends on whether its subsequence DTW\n",
    "    # essentially allow us to hop on the bottom anywhere (so could start partway through one of the signals)\n",
    "    if parameter['SubSequence']:\n",
    "        for col in range(numCols):\n",
    "            accumCost[maxRowStep, col + maxColStep] = C[0, col]\n",
    "    else:\n",
    "        accumCost[maxRowStep, maxColStep] = C[0,0]\n",
    "\n",
    "    # filling the accumulated cost matrix\n",
    "    for row in range(maxRowStep, numRows + maxRowStep, 1):\n",
    "        for col in range(maxColStep, numCols + maxColStep, 1):\n",
    "            bestCost = accumCost[<unsigned int>row, <unsigned int>col] # initialize with what's there - so if is an entry point, then can start low\n",
    "            bestCostIndex = 0\n",
    "            # go through each step, find the best one\n",
    "            for stepIndex in range(numDifSteps):\n",
    "                #costForStep = accumCost[<unsigned int>(row - dn[<unsigned int>(stepIndex)]), <unsigned int>(col - dm[<unsigned int>(stepIndex)])] + dw[<unsigned int>(stepIndex)] * C[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)]\n",
    "                costForStep = accumCost[<unsigned int>((row - dn[(stepIndex)])), <unsigned int>((col - dm[(stepIndex)]))] + dw[stepIndex] * C[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)]\n",
    "                if costForStep < bestCost:\n",
    "                    bestCost = costForStep\n",
    "                    bestCostIndex = stepIndex\n",
    "            # save the best cost and best cost index\n",
    "            accumCost[row, col] = bestCost\n",
    "            steps[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)] = bestCostIndex\n",
    "\n",
    "    # return the accumulated cost along with the matrix of steps taken to achieve that cost\n",
    "    return [accumCost[maxRowStep:, maxColStep:], steps]\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def DTW_GetPath(np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost, np.ndarray[np.uint32_t, ndim=2] stepsForCost, parameter):\n",
    "    '''\n",
    "\n",
    "    Parameter should have: 'dn', 'dm', 'dw', 'SubSequence'\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dn\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dm\n",
    "    cdef np.uint8_t subseq\n",
    "    # make sure dn, dm, and dw are setup\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        dn = parameter['dn']\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=DTYPE_INT32)\n",
    "    if 'dm'  in parameter.keys():\n",
    "        dm = parameter['dm']\n",
    "    else:\n",
    "        dm = np.array([1, 0, 1], dtype=DTYPE_INT32)\n",
    "    if 'SubSequence' in parameter.keys():\n",
    "        subseq = parameter['SubSequence']\n",
    "    else:\n",
    "        subseq = 0\n",
    "\n",
    "    cdef np.uint32_t numRows\n",
    "    cdef np.uint32_t numCols\n",
    "    cdef np.uint32_t curRow\n",
    "    cdef np.uint32_t curCol\n",
    "    cdef np.uint32_t endCol\n",
    "    cdef DTYPE_FLOAT_t endCost\n",
    "\n",
    "    numRows = accumCost.shape[0]\n",
    "    numCols = accumCost.shape[1]\n",
    "\n",
    "    # either start at the far corner (non sub-sequence)\n",
    "    # or start at the lowest cost entry in the last row (sub-sequence)\n",
    "    # where all of the signal along the row has been used, but only a \n",
    "    # sub-sequence of the signal along the columns has to be used\n",
    "    curRow = numRows - 1\n",
    "    if subseq:\n",
    "        curCol = np.argmin(accumCost[numRows - 1, :])\n",
    "    else:\n",
    "        curCol = numCols - 1\n",
    "\n",
    "    endCol = curCol\n",
    "    endCost = accumCost[curRow, curCol]\n",
    "\n",
    "    cdef np.uint32_t curRowStep\n",
    "    cdef np.uint32_t curColStep\n",
    "    cdef np.uint32_t curStepIndex\n",
    "\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=2] path = np.zeros((2, numRows + numCols), dtype=np.uint32) # make as large as could need, then chop at the end\n",
    "    path[0, 0] = curRow\n",
    "    path[1, 0] = curCol\n",
    "\n",
    "    cdef np.uint32_t stepsInPath = 1 # starts at one, we add in one before looping\n",
    "    cdef np.uint32_t stepIndex = 0\n",
    "    cdef np.int8_t done = (subseq and curRow == 0) or (curRow == 0 and curCol == 0)\n",
    "    while not done:\n",
    "        if accumCost[curRow, curCol] == MAX_FLOAT:\n",
    "            print('A path is not possible')\n",
    "            break\n",
    "\n",
    "        # you're done if you've made it to the bottom left (non sub-sequence)\n",
    "        # or just the bottom (sub-sequence)\n",
    "        # find the step size\n",
    "        curStepIndex = stepsForCost[curRow, curCol]\n",
    "        curRowStep = dn[curStepIndex]\n",
    "        curColStep = dm[curStepIndex]\n",
    "        # backtrack by 1 step\n",
    "        curRow = curRow - curRowStep\n",
    "        curCol = curCol - curColStep\n",
    "        # add your new location onto the path\n",
    "        path[0, stepsInPath] = curRow\n",
    "        path[1, stepsInPath] = curCol\n",
    "        stepsInPath = stepsInPath + 1\n",
    "        # check to see if you're done\n",
    "        done = (subseq and curRow == 0) or (curRow == 0 and curCol == 0)\n",
    "\n",
    "    # reverse the path (a matrix with two rows) and return it\n",
    "    return [np.fliplr(path[:, 0:stepsInPath]), endCol, endCost]\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAlignment(D, wp, seginfo = None):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(D, origin = 'lower', cmap = 'jet')\n",
    "    plt.plot(wp[:,1], wp[:,0], color='y')\n",
    "    plt.xlabel('Ref')\n",
    "    plt.ylabel('Query')\n",
    "    if seginfo is not None:\n",
    "        matchSegTime, refSegTimes, refSegCols = seginfo\n",
    "        for i, refSegCol in enumerate(refSegCols):            \n",
    "            plt.axvline(refSegCol[0], color = 'm')\n",
    "            plt.axvline(refSegCol[1], color = 'm')\n",
    "        plt.title('Hyp ({:.1f} s, {:.1f} s), Ref ({:.1f} s, {:.1f} s)'.format(matchSegTime[0], matchSegTime[1], refSegTimes[0][0], refSegTimes[0][1]))\n",
    "    else:\n",
    "        plt.title('Subsequence DTW Alignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictedTimestamps(wp, times):\n",
    "    start_frm_midi = wp[-1,1]\n",
    "    end_frm_midi = wp[0,1]\n",
    "    start_time_midi = times[start_frm_midi][0] # in sec\n",
    "    end_time_midi = times[end_frm_midi][0]\n",
    "    start_tick_midi = times[start_frm_midi][1] # in ticks\n",
    "    end_tick_midi = times[end_frm_midi][1]\n",
    "    return (start_time_midi, end_time_midi), (start_tick_midi, end_tick_midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruthTimestamps(imgfile, col2times):\n",
    "    \n",
    "    # get ground truth start, end times\n",
    "    query = os.path.splitext(os.path.basename(imgfile))[0] # e.g. '/path/to/dir/p1_q10.jpg'\n",
    "    query_gt_file = 'data/query_info/query.gt'\n",
    "    refmatchesTime = []\n",
    "    with open(query_gt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(',')\n",
    "            if parts[0] == query:\n",
    "                tstart = float(parts[1])\n",
    "                tend = float(parts[2])\n",
    "                refmatchesTime.append((tstart, tend))\n",
    "    \n",
    "    # get start, end columns in bootleg score\n",
    "    bscore_cols = np.arange(len(col2times))\n",
    "    times = [tup[0] for tup in col2times]\n",
    "    refmatchesCol = []\n",
    "    for (tstart, tend) in refmatchesTime:\n",
    "        col_start, col_end = np.interp([tstart, tend], times, bscore_cols)\n",
    "        refmatchesCol.append((col_start, col_end))\n",
    "    \n",
    "    return refmatchesTime, refmatchesCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieceStr = os.path.basename(imagefile).split('_')[0]\n",
    "midi_bscore_pkl = '{}/{}.pkl'.format(midi_db_dir, pieceStr)\n",
    "bscore_midi, miditimes, num_notes, stafflines = loadMidiBootlegScore(midi_bscore_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, wp = alignBootlegScores(bscore_query, bscore_midi, num_notes, dtw_steps, dtw_weights)\n",
    "matchSegmentTime, matchSegmentTick = getPredictedTimestamps(wp, miditimes)\n",
    "refSegmentTimes, refSegmentCols = getGroundTruthTimestamps(imagefile, miditimes)\n",
    "plotAlignment(D, wp, (matchSegmentTime, refSegmentTimes, refSegmentCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeAlignedBScores(s1, s2, wp, lines):\n",
    "    idxs1 = wp[::-1, 0]\n",
    "    warped1 = s1[:,idxs1]\n",
    "    idxs2 = wp[::-1, 1]\n",
    "    warped2 = s2[:,idxs2]\n",
    "    stacked = np.vstack((warped2, warped1))\n",
    "    allLines = []\n",
    "    allLines.extend(lines)\n",
    "    allLines.extend(np.array(lines) + s1.shape[0])\n",
    "    visualizeLongBootlegScore(stacked, allLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeAlignedBScores(bscore_query, bscore_midi, wp, stafflines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLongBootlegScore(bscore_midi_refseg, staffLinesBoth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize alignment between query and ground truth reference segment\n",
    "refColStart = int(refSegmentCols[0][0])\n",
    "refColEnd = int(refSegmentCols[0][1])\n",
    "bscore_midi_refseg = bscore_midi[:,refColStart:refColEnd]\n",
    "D2, wp2 = alignBootlegScores(bscore_query, bscore_midi_refseg, num_notes[refColStart:refColEnd])\n",
    "visualizeAlignedBScores(bscore_query, bscore_midi_refseg, wp2, stafflines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run system on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQuery(imagefile, midi_bscore_pkl, outfile = None):\n",
    "\n",
    "    ### system parameters ###\n",
    "    \n",
    "    # Pre-processing\n",
    "    #resizeW = 1000 # initial resize\n",
    "    #resizeH = 1000\n",
    "    thumbnailW = 100  # bkgd lighting\n",
    "    thumbnailH = 100\n",
    "    thumbnailFilterSize = 5\n",
    "    estLineSep_NumCols = 3\n",
    "    estLineSep_LowerRange = 25\n",
    "    estLineSep_UpperRange = 45\n",
    "    estLineSep_Delta = 1\n",
    "    targetLineSep = 10.0\n",
    "\n",
    "    # Staff Line Features\n",
    "    morphFilterHorizLineSize = 51\n",
    "    notebarFiltLen = 3\n",
    "    notebarRemoval = 0.9\n",
    "    calcStaveFeatureMap_NumCols = 10\n",
    "    calcStaveFeatureMap_LowerRange = 8.5\n",
    "    calcStaveFeatureMap_UpperRange = 11.75\n",
    "    calcStaveFeatureMap_Delta = 0.25\n",
    "\n",
    "    # Notehead Detection\n",
    "    morphFilterCircleSizeReduce = 5\n",
    "    morphFilterCircleSizeExpand = 5\n",
    "    #morphFilterCircleSize = 5\n",
    "    notedetect_minarea = 50\n",
    "    notedetect_maxarea = 200\n",
    "    noteTemplateSize = 21\n",
    "    notedetect_tol_ratio = .4\n",
    "    chordBlock_minH = 1.8\n",
    "    chordBlock_maxH = 4.25\n",
    "    chordBlock_minW = .8\n",
    "    chordBlock_maxW = 1.25\n",
    "    chordBlock_minArea = 1.8\n",
    "    chordBlock_maxArea = 4.5\n",
    "    chordBlock_minNotes = 2\n",
    "    chordBlock_maxNotes = 4\n",
    "\n",
    "    # Staffline Detection\n",
    "    maxDeltaRowInitial = 50\n",
    "    minNumStaves = 2\n",
    "    maxNumStaves = 12\n",
    "    minStaveSeparation = 6 * targetLineSep\n",
    "    maxDeltaRowRefined = 15\n",
    "\n",
    "    # Group Staves\n",
    "    morphFilterVertLineLength = 101\n",
    "    morphFilterVertLineWidth = 7\n",
    "    #maxBarlineLenFactor = .25\n",
    "    #maxBarlineWidth = 10\n",
    "\n",
    "    # Generate Bootleg Score\n",
    "    bootlegRepeatNotes = 2 \n",
    "    bootlegFiller = 1\n",
    "\n",
    "    # Alignment\n",
    "    dtw_steps = [1,1,1,2,2,1] # dtw\n",
    "    dtw_weights = [1,1,2]\n",
    "\n",
    "    ##########################\n",
    "    \n",
    "    print(\"Processing {}\".format(imagefile))\n",
    "    profileStart = time.time()\n",
    "\n",
    "    # pre-processing\n",
    "    pim1 = Image.open(imagefile).convert('L') # pim indicates PIL image object, im indicates raw pixel values\n",
    "    #pim1 = pim1.resize([resizeW, resizeH])\n",
    "    pim2 = removeBkgdLighting(pim1, thumbnailFilterSize, thumbnailW, thumbnailH)\n",
    "    linesep, scores = estimateLineSep(pim2, estLineSep_NumCols, estLineSep_LowerRange, estLineSep_UpperRange, estLineSep_Delta)\n",
    "    targetH, targetW = calcResizedDimensions(pim2, linesep, targetLineSep)\n",
    "    pim2 = pim2.resize((targetW, targetH))\n",
    "\n",
    "    # staff line features\n",
    "    X2 = getNormImage(pim2)\n",
    "    hlines = isolateStaffLines(X2, morphFilterHorizLineSize, notebarFiltLen, notebarRemoval)\n",
    "    featmap, stavelens, columnWidth = computeStaveFeatureMap(hlines, calcStaveFeatureMap_NumCols, calcStaveFeatureMap_LowerRange, calcStaveFeatureMap_UpperRange, calcStaveFeatureMap_Delta)\n",
    "    \n",
    "    # notehead detection\n",
    "    im3 = morphFilterCircle(pim2, morphFilterCircleSizeReduce, morphFilterCircleSizeExpand)\n",
    "    keypoints, im_with_keypoints = detectNoteheadBlobs(im3, notedetect_minarea, notedetect_maxarea)\n",
    "    X3 = getNormImage(im3) # im indicates grayscale [0, 255], X indicates [0, 1] inverted grayscale\n",
    "    ntemplate, numCrops = getNoteTemplate(X3, keypoints, noteTemplateSize)\n",
    "    chordBlockSpecs = (chordBlock_minH, chordBlock_maxH, chordBlock_minW, chordBlock_maxW, chordBlock_minArea, chordBlock_maxArea, chordBlock_minNotes, chordBlock_maxNotes)\n",
    "    notes, img_binarized_notes = adaptiveNoteheadDetect(X3, ntemplate, notedetect_tol_ratio, chordBlockSpecs)\n",
    "    if len(notes) == 0: # if no notes detected, stop early\n",
    "        saveToFile(outfile, imagefile, (0, 0), time.time() - profileStart)\n",
    "        return (0,0)\n",
    "    nhlocs, nhlen_est, nhwidth_est = getNoteheadInfo(notes)\n",
    "    \n",
    "    # infer note values\n",
    "    estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowInitial, int(-2*targetLineSep))\n",
    "    staveMidpts = estimateStaffMidpoints(estStaffLineLocs, minNumStaves, maxNumStaves, minStaveSeparation)\n",
    "    staveIdxs, nhRowOffsets = assignNoteheadsToStaves(nhlocs, staveMidpts)\n",
    "    estStaffLineLocs, sfiltlen = getEstStaffLineLocs(featmap, nhlocs, stavelens, columnWidth, maxDeltaRowRefined, (nhRowOffsets - 2*targetLineSep).astype(np.int))    \n",
    "    nhvals = estimateNoteLabels(estStaffLineLocs)\n",
    "    \n",
    "    # cluster noteheads & staves\n",
    "    vlines = isolateBarlines(X2, morphFilterVertLineLength, morphFilterVertLineWidth)\n",
    "    staveMapping, evidence = determineStaveGrouping(staveMidpts, vlines)\n",
    "    nhclusters, clusterPairs = clusterNoteheads(staveIdxs, staveMapping)\n",
    "    \n",
    "    # generate & align bootleg scores\n",
    "    nhdata = [(int(np.round(nhlocs[i][0])), int(np.round(nhlocs[i][1])), nhvals[i], nhclusters[i]) for i in range(len(nhlocs))]\n",
    "    bscore_query, events, eventIndices, staffLinesBoth = generateQueryBootlegScore(nhdata, clusterPairs, bootlegRepeatNotes, bootlegFiller, minColDiff = nhwidth_est)\n",
    "    bscore_midi, miditimes, num_notes, stafflines = loadMidiBootlegScore(midi_bscore_pkl)\n",
    "    D, wp = alignBootlegScores(bscore_query, bscore_midi, num_notes, dtw_steps, dtw_weights)\n",
    "    matchSegmentTime, matchSegmentTick = getPredictedTimestamps(wp, miditimes)\n",
    "        \n",
    "    # profile & save to file\n",
    "    profileEnd = time.time()\n",
    "    profileDur = profileEnd - profileStart\n",
    "    saveToFile(outfile, imagefile, matchSegmentTime, profileDur)\n",
    "        \n",
    "    return matchSegmentTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(outfile, imagefile, segment, dur):\n",
    "    if outfile:\n",
    "        with open(outfile, 'w') as f:\n",
    "            query = os.path.splitext(os.path.basename(imagefile))[0]\n",
    "            outStr = \"{},{:.2f},{:.2f},{:.2f}\\n\".format(query, segment[0], segment[1], dur)\n",
    "            f.write(outStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # process single query\n",
    "# query_file = 'data/queries/p1_q1.jpg'\n",
    "# pieceStr = os.path.basename(query_file).split('_')[0]\n",
    "# midi_bscore_file = 'experiments/baseline/db/{}.pkl'.format(pieceStr)\n",
    "# matchTimes = processQuery(query_file, midi_bscore_file)\n",
    "# matchTimes\n",
    "# #cProfile.run(\"processQuery(query_file, midi_bscore_file)\") # for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQuery_wrapper(queryfile, mididir, outdir):\n",
    "    # wrapper for running multiple jobs in parallel\n",
    "    basename = os.path.splitext(os.path.basename(queryfile))[0] # e.g. p1_q1\n",
    "    hyp_outfile = \"{}/{}.hyp\".format(outdir, basename)\n",
    "    piece = basename.split('_')[0]\n",
    "    midiBootlegFile = \"{}/{}.pkl\".format(mididir, piece)\n",
    "    return processQuery(queryfile, midiBootlegFile, hyp_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all queries\n",
    "query_list = 'cfg_files/query.train.list' # list of query images\n",
    "midi_bs_dir = 'experiments/baseline/db' # directory containing midi bootleg scores\n",
    "outdir = 'experiments/baseline/hyp' # where to save hypothesis output files\n",
    "\n",
    "# prep output directory\n",
    "if not os.path.isdir(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# number of cores to use\n",
    "n_cores = 32 #multiprocessing.cpu_count()\n",
    "\n",
    "# prep inputs for parallelization\n",
    "inputs = []\n",
    "with open(query_list, 'r') as f:\n",
    "    for line in f:\n",
    "        inputs.append((line.rstrip(), midi_bs_dir, outdir))\n",
    "\n",
    "# process queries in parallel\n",
    "pool = multiprocessing.Pool(processes=n_cores)\n",
    "outputs = list(pool.starmap(processQuery_wrapper, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SheetMidiRetrieval",
   "language": "python",
   "name": "sheetmidiretrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
